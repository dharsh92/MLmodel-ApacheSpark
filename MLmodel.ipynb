{
  "metadata": {
    "name": "MLmodel",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n#Reading the CSV file and adding it to a dataframe\n\nmatches \u003d spark.read.csv(\"/data/results.csv\",header\u003d\"true\" , inferSchema\u003d\"true\")\nmatches.show()\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "\n%pyspark\n\n#creating a new column called result with the below outputs.\n\n# value 1- when the home team wins\n# value 2 - when the away team wins\n# value 0 - when the match is draw\n\nfrom pyspark.sql.functions import col, when\n\n\nmatches \u003d matches.withColumn(\"result\", when(col(\"home_score\") \u003e col(\"away_score\"),1)\n                                 .when(col(\"home_score\") \u003c col(\"away_score\"),2)\n                                 .otherwise(\"0\"))\n\nmatches.show(truncate\u003dFalse)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\nfrom pyspark.sql.functions import col, when\n\n#removing the matches that were drawn\n\nmatches\u003dmatches.filter(matches.result!\u003d0)\nmatches.show()\n\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n\n#removing date,city,tournament,neutral as those columns are not required.\n\nmatches\u003dmatches.drop(\"date\").drop(\"city\").drop(\"tournament\").drop(\"neutral\").drop(\"home_score\").drop(\"away_score\")\nmatches.show()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n# result is a string data type\n\nmatches.select(\u0027result\u0027)\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\ncategoricalColumns \u003d [item[0] for item in matches.dtypes if item[1].startswith(\u0027string\u0027) ]\n\ncategoricalColumns"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n\n#Converting String values columns into indexes\n\nfrom pyspark.ml.feature import StringIndexer,VectorIndexer\n\nlabelIndexer \u003d StringIndexer(inputCol\u003d\"home_team\", outputCol\u003d\"home_team_indexed\").fit(matches).transform(matches)\n\nlabelIndexer \u003d StringIndexer(inputCol\u003d\"away_team\", outputCol\u003d\"away_team_indexed\").fit(labelIndexer).transform(labelIndexer)\n\nlabelIndexer \u003d StringIndexer(inputCol\u003d\"country\", outputCol\u003d\"country_indexed\").fit(labelIndexer).transform(labelIndexer)\n\nlabelIndexer \u003d StringIndexer(inputCol\u003d\"result\", outputCol\u003d\"label\").fit(labelIndexer).transform(labelIndexer)\n\nlabelIndexer.show()\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "\n%pyspark\n\n#home_team_indexed,home_team_indexed,home_team_indexed will be the features.\n\n# below code will convert the index values to vectors.\n\nfrom pyspark.ml.feature import VectorAssembler\n\ncols \u003d [\"home_team_indexed\",\"away_team_indexed\",\"country_indexed\"]\n\nassembler \u003d VectorAssembler(inputCols\u003dcols,outputCol\u003d\"selectedfeatures\")\n\noutput \u003d assembler.transform(labelIndexer)\n\n# the label column convert values in the result column to 0 and 1\n# which means 0 refers to home team wins (1-\u003e0)\n# and 1 refers to the away team wins(2-\u003e1)\noutput.show()\n\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n\n#converting the selected featured into scaled features for better result.\n\nfrom pyspark.ml.feature import StandardScaler\n\nscaler  \u003d StandardScaler(inputCol\u003d\"selectedfeatures\", outputCol\u003d\"features\",\n                        withStd\u003dTrue, withMean\u003dFalse)\nscalerModel \u003d scaler.fit(output)\n\nscaledData \u003d scalerModel.transform(output)\nscaledData.show()\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n\n# we are passing the below label and features to the model\n\nscaledData\u003dscaledData.drop(\"home_score\").drop(\"away_score\").drop(\"country\").drop(\"home_team\").drop(\"away_team\").drop(\"selectedfeatures\").drop(\"home_team_indexed\").drop(\"away_team_indexed\").drop(\"country_indexed\").drop(\"result\")\nscaledData.show()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n\n# splitting the data into testing and training: 80% for training data: 20% testing data\n\nfrom pyspark.ml.classification import LogisticRegression\n\n\n(trainingData, testData) \u003d scaledData.randomSplit([0.8, 0.2])"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n\n#fit the model with maximum 100 iterations.\n\nfrom pyspark.ml.classification import LogisticRegression\n\nlr \u003d LogisticRegression(maxIter\u003d200, regParam\u003d0.02, elasticNetParam\u003d0.8)\n\n# Fit the model\nprint(\"Model Training Started!\")\nlrModel \u003d lr.fit(trainingData)\nprint(\"Model Training Stopped!\")"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n#we can see that the label and prediction column has the same value in most rows.\n\npredictiondf \u003d lrModel.transform(testData)\npredictiondf.drop(\"home_team_indexed\").drop(\"away_team_indexed\").drop(\"country_indexed\").drop(\"features\")\npredictiondf.show(1000)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n\n# the accuracy of the model is 0.62\n\n\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\nev\u003d MulticlassClassificationEvaluator()\nprint(\u0027accuracy\u0027, ev.evaluate(predictiondf))\n"
    }
  ]
}